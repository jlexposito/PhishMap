---
title: "Report Phismap"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
options(width=180)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Resumen

Este informe realiza un anális sobre una lista de URLs marcadas públicamente como phising. La fuente de estas IPs es [Phistank](https://www.phishtank.com/). Concretamente, se quiere conocer la respuesta a las siguientes preguntas:

- ¿Cuál país aloja la mayor cantidad de Phising?

- ¿En qué proporción se distribuyen estas URLs entre todos los países?

- ¿Cuáles son las direcciones o rangos de direcciones IP más utilizadas para alojar Phising?

- ¿Cuáles son los dominios más utilizados para alojar Phising?


## Análisis

```{r load_packages, echo = FALSE, cache = FALSE}
# Paquetes necesarios
pkg <- c("jsonlite", "rworldmap", "RColorBrewer", "RCurl", "ggplot2", "dplyr", "stringr")

# Instalar paquetes que no estén instalados
# Instalar paquetes que no esten instalados
new.pkg <- pkg[!(pkg %in% installed.packages())]
if (length(new.pkg)) 
{
    install.packages(new.pkg)  
}

library(jsonlite)
library(rworldmap)
library(RColorBrewer)
library(RCurl)
library(ggplot2)
library(dplyr)
library(stringr)
library(parallel)
library(maps)
require(ggmap)
library(plotly)

```

Phistank proporciona una serie de URLs las cuales son reportadas por diferentes usuarios a nivel mundial. Estas URLs son mantenidas en este repositorio mientras permanezcan en línea.
Phistank permite descargar un archivo con esta información así como una serie de metadatos, entre los que se incluyen el país de origen del dominio presente en la URL, dirección IP, ASN, etc.

```{r load_phistank_data, echo = FALSE, cache = TRUE}
#Download data file
download.file("http://data.phishtank.com/data/online-valid.json.bz2",destfile="data.json",method="libcurl")
file = "data.json"
data.raw <- fromJSON(file)$details
data.dataFrame <- do.call(rbind.data.frame, data.raw)
keep <- c("ip_address", "cidr_block", "country")
data.dataFrame <- data.dataFrame[keep]

#Create Countries data frame
data.dataFrame <- dplyr::filter(data.dataFrame, country != '')
countries <- unlist(data.dataFrame$country)
countries_tables <- as.data.frame(table(countries))
countries_tables <- countries_tables[ order(-countries_tables[,2], countries_tables[,1]), ]

# Top FQDNs
URLs <- as.data.frame(fromJSON(file)$url)
colnames(URLs) <- c("URLs")
FQDNs <- str_match(URLs$URLs, "^(?:https?://)?([^/]+)")
domains <- as.data.frame(table(FQDNs[,2]))
#Find most used Domains
domains <- domains[ order(-domains[,2], domains[,1]), ]
colnames(domains) <- c("Domains", "Appearances")

google_domains <- domains[grep("google", domains$Domains, ignore.case = TRUE), ]
```

Los datos iniciales tienen la siguiente estructura:

```{r initial_data, echo = FALSE}
head(data.dataFrame)
```

Como puede verse, los países son identificados utilizando la norma **ISO 3166**, por lo que utilizamos en nuestro código este estándar, evitando errores al momento de leer los datos. Igualmente, se utiliza una paleta de colores personalizada para poder visualizar de mejor manera los datos en el mapa, al igual que una escala logarítmica pues el número de coincidencias por país difiere enormemente (esto se verá con detalle más adelante).

```{r create_map, echo=TRUE, message= FALSE}
#Generate MapData
map_data <- joinCountryData2Map(countries_tables, joinCode="ISO2", 
                            nameJoinColumn="countries", verbose = F)
#Change MapColorPalette
map_color_palette <- RColorBrewer::brewer.pal(n = 7, name = "YlOrRd")
map_country_data <- mapCountryData(map_data, mapTitle = "PhishMap", nameColumnToPlot="Freq", 
               catMethod = "logFixedWidth", colourPalette = map_color_palette, 
               addLegend = F, lwd = 1, borderCol = 'black')

```

Representando la información por país, se puede observar que la gran mayoría de las URLs apuntan a direcciones IP localizadas en Estados Unidos, China y Europa en general.

Ahora tenemos una representación del número real de URLs alojadas en los primeros diez países. Podemos confirmar que, en efecto, el mayor número de las mismas corresponden a Estados Unidos por un amplio margen.

```{r top_ten_countries, echo=FALSE}
#Top 10 table
top_ten_countries <- head(countries_tables, 10)

#Top 10 bars graphic
top_ten_countries_bar_plot <- ggplot(
                                data=top_ten_countries, 
                                aes(x=countries, y=Freq, fill=countries)
                              ) +
                              guides(fill=FALSE) +
                              xlab("Countries") + ylab("Phishing sites") +
                              geom_bar(stat="identity", position = 'dodge') + theme_grey() +
                              geom_text(aes(label=Freq), position=position_dodge(width=1), vjust=-0.6)
                              
top_ten_countries_bar_plot + scale_fill_brewer(palette="RdBu", 
                                               name = "Country code \n(ISO2) ",
                                               type="seq")
```

Podemos igualmente visualizar cuáles son los bloques CIDR más utilizados, buscando identificar algún tipo de indicio que muestre si estas IPs están ubicadas en grupos de Data Centers específicos.

Igualmente, se puede tomar únicamente el FQDN de cada URL y agruparlas, buscando identificar a través de qué dominios se ejecutan estas campañas de Phising. El resultado muestra que se hace un uso considerable de acortadores de URL como **bit.ly.** Por otra parte, se aprecia un uso recurrente de diferentes dominios o sub-dominios relacionados con Google, como por ejemplo, Google Docs o Google Drive, entre otros.
```{r top_ten_fqdns_calculation, echo = FALSE}
head(domains)
```
```{r google_sites, echo = FALSE}
head(google_domains)
```

Debido a la amplia predominancia de Estados Unidos con respecto al total de URLs de phising, se consideró adecuado hilar más fino en estas y visualizar los datos por estado y ciudades. Sin embargo, vale la pena mencionar que se encontró una serie de problemas al momento de realizar la geolocalización de las IPs:

- El dataset que permite realizar el *mapping* de direcciones IP a estados y, sobre todo, ciudades, es muy grande. Del orden de millones de entradas.
- El número de IPs a geolocalizar también es elevado. En la práctica, la eficiencia de la búsqueda de todas las IPs estaría en el orden de O(n*m), con entradas muy grandes.
- La función para verificar que una IP esté dentro de un rango específico no está optimizada.

En conjunto, todos estos puntos volvieron inviable la ejecución del proyecto sin incluir paralelismo, por lo que fue incluido en nuestro código. Durante nuestra pruebas, realizamos diferentes experimentos variando el número de cores utilizados para el procesamiento de los datos, encontrando como hecho curioso que utilizando todos los cores de la máquina traía en algunos casos resultados incluso peores que cuando se excluía un core. Se llegó a la conclusión de que al no disponer R de un core libre para realizar la orquestación necesaria de los diferentes sub-procesos creados, se generaba un cuello de botella.

Por otra parte, se realizaron pruebas en una máquina más potente con un número más elevado de cores; sin embargo, nos encontramos con un problema diferente y es que el ordenador se quedó sin memoria disponible. Concluimos que esto se debe a que cada subproceso tiene en su espacio de memoria una copia por separado del dataframe con la información de geolocalización. Por esta razón, al aumentar demasiado el número de cores, se agota con mucha más rapidez la memoria del equipo.

```{r geolocation, echo=FALSE, cache = TRUE}
#Create IP data frame
data.dataFrame <- dplyr::filter(data.dataFrame, ip_address != '' )
usa_data <- dplyr::filter(data.dataFrame, country == 'US')

ip_tables <- as.data.frame(usa_data$ip, stringsAsFactors = F)
colnames(ip_tables) <- c("ip")

ip_tables <- as.data.frame(str_match(ip_tables$ip, "(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(?:[.]|$)){4}"), stringsAsFactors = F)
colnames(ip_tables) <- c("ip")

ip_tables <- dplyr::filter(ip_tables, !is.na(ip))

#Find mot used IP addresses
ip_tables <- count(ip_tables,ip, sort = T)

#### Custom functions
ip2long <- function(ip) {
  # transforma a vector de characters
  ips <- unlist(strsplit(ip, '.', fixed = TRUE))
  # set up a function to bit-shift, then "OR" the octets
  octet <- function(x,y) bitops::bitOr(bitops::bitShiftL(x, 8), y)
  # Reduce applys a function cumulatively left to right
  return(Reduce(octet, as.integer(ips)))
}

find.cityState <- function(ip) {
  t <- dplyr::filter(ip2country, ip >= block_start_long & ip <= block_end_long)
  return ( dplyr::select_(t, .dots = c("city", "state") ) )
}

#### IP blocks by country, (http://download.db-ip.com/free/dbip-city-2017-05.csv.gz)
ip2countries.url <- "http://download.db-ip.com/free/dbip-city-2017-05.csv.gz"
download.file(url = ip2countries.url, destfile = "./cities.csv")
ip2country <- read.csv("./cities.csv", header = F, stringsAsFactors = F)

names(ip2country) <- c("block_start", "block_end", "country", "state", "city")
ip2country <- dplyr::filter(ip2country, country == "US")

#### extract ip's & compute numeric equivalent
ip_tables$ip_long <- sapply(X = ip_tables$ip, ip2long)

#### compute numeric equivalent for ip blocks
# Calculate the number of cores
no_cores <- detectCores() -1

# Initiate cluster
cl <- makeCluster(no_cores)

#<cache> true
ip2country$block_start_long <- parSapply(cl, X = ip2country$block_start, FUN = ip2long)
ip2country <- dplyr::filter(ip2country, !is.na(block_start_long))

# Parallelized: ~3m 30s
ip2country$block_end_long <-  parSapply(cl, ip2country$block_end, FUN = ip2long)
ip2country <- dplyr::filter(ip2country, !is.na(block_end_long))

#</cache> 

# Compute Aggregates -----------------------------------------------------------
clusterExport(cl, "ip2country")

ip_tables100 <- head(ip_tables, 50)
cityState <- parSapply(cl, ip_tables100$ip_long, FUN = find.cityState)
cityState <- t(cityState)
colnames(cityState) <- c("city", "state")
cityState <- as.data.frame(cityState)
keep <- c("city", "state")
cityState <- cityState[keep]

ip_tables100$city <- cityState$city
ip_tables100$state <- tolower(cityState$state)

#ip_tables['city'] <- as.factor(ip_tables$city)
ip_tables100 <- dplyr::filter(ip_tables100, !is.na(city))

stopCluster(cl)

unique(ip_tables100$city)
unique(ip_tables100$state)

# Compute Aggregates -----------------------------------------------------------
ip_tables100.aggregateState <- as.data.frame(table(ip_tables100$state))

all_states <- map_data("state")
colnames(ip_tables100.aggregateState) <- c("region", "Freq")
ip_tables100.aggregateState$region <- tolower(ip_tables100.aggregateState$region)
Total <- merge(all_states, ip_tables100.aggregateState, by.x="region")
```
